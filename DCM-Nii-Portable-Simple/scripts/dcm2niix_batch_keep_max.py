#!/usr/bin/env python3
"""
DCM-Nii æ‰¹é‡å¤„ç†è„šæœ¬ - ç®€åŒ–ç¨³å®šç‰ˆ
è‡ªåŠ¨å¤„ç†DICOMè½¬NIfTI + å…ƒæ•°æ®å¯¼å‡º + è„±æ•
"""
import subprocess
import os
import glob
import nibabel as nib
import shutil
import pydicom
from collections import defaultdict
import tempfile
import sys
import zipfile
import csv

# é…ç½®å‚æ•°
DCM2NIIX_PATH = r"D:\git\DCM-Nii\tools\MRIcroGL\Resources\dcm2niix.exe"
OUTPUT_DIR = r"D:\git\DCM-Nii\output"

# æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
if len(sys.argv) > 1:
    DATA_DIR = sys.argv[1]
    print(f"ä½¿ç”¨æŒ‡å®šç›®å½•: {DATA_DIR}")
else:
    DATA_DIR = r"D:\git\DCM-Nii\data"
    print(f"ä½¿ç”¨é»˜è®¤ç›®å½•: {DATA_DIR}")

# æ£€æŸ¥dcm2niixå·¥å…·
if not os.path.exists(DCM2NIIX_PATH):
    DCM2NIIX_PATH = r"D:\git\DCM-Nii\dcm2niix.exe"
    if not os.path.exists(DCM2NIIX_PATH):
        print("âŒ æœªæ‰¾åˆ° dcm2niix.exeï¼Œè¯·ç¡®è®¤å·¥å…·è·¯å¾„")
        sys.exit(1)

print(f"ä½¿ç”¨è½¬æ¢å·¥å…·: {DCM2NIIX_PATH}")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# å…ƒæ•°æ®ç›¸å…³é…ç½®
META_CSV = os.path.join(OUTPUT_DIR, 'case_metadata.csv')
MASKED_CSV = os.path.join(OUTPUT_DIR, 'case_metadata_masked.csv')
FIELDS = [
    'FileName', 'PatientName', 'PatientID', 'StudyDate', 'PatientBirthDate', 'PatientAge', 'PatientSex',
    'StudyInstanceUID', 'SeriesInstanceUID', 'Modality', 'Manufacturer', 'Rows', 'Columns', 'ImageCount', 'SeriesCount'
]

metadata_rows = []
masked_rows = []

def desensitize_name(name):
    """è„±æ•æ‚£è€…å§“å"""
    if not name:
        return ''
    if hasattr(name, 'family_name') or hasattr(name, 'given_name'):
        name = str(name)
    name = str(name).strip()
    if not name:
        return ''
    first_letter = name[0].upper()
    if ' ' in name:
        return first_letter + '**'
    else:
        return first_letter + '*'

def is_dicom_file(filepath):
    """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºDICOMæ–‡ä»¶"""
    try:
        filename = os.path.basename(filepath).lower()
        dicom_extensions = ['.dcm', '.dicom', '.dic', '.ima']
        if any(filename.endswith(ext) or filename.endswith(ext.upper()) for ext in dicom_extensions):
            return True
        
        if filename.startswith(('img', 'im_', 'i')) or filename.isdigit():
            with open(filepath, 'rb') as f:
                f.seek(128)
                header = f.read(4)
                if header == b'DICM':
                    return True
        
        if any(pattern in filename for pattern in ['dicom', 'scan', 'slice']):
            return True
            
        return False
    except:
        return False

def find_dicom_files(directory):
    """é€’å½’æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰DICOMæ–‡ä»¶"""
    dicom_files = []
    print(f"æ­£åœ¨æ‰«æç›®å½•: {directory}")
    
    for root, dirs, files in os.walk(directory):
        dirs[:] = [d for d in dirs if not d.startswith('.') and d.lower() not in ['__pycache__', 'logs']]
        
        for file in files:
            filepath = os.path.join(root, file)
            if is_dicom_file(filepath):
                dicom_files.append(filepath)
    
    return dicom_files

def extract_zip_files(data_dir):
    """è‡ªåŠ¨è§£å‹ç›®å½•ä¸‹çš„ZIPæ–‡ä»¶"""
    zip_files = []
    extracted_dirs = []
    
    for item in os.listdir(data_dir):
        item_path = os.path.join(data_dir, item)
        if os.path.isfile(item_path) and item.lower().endswith('.zip'):
            zip_files.append(item_path)
    
    if zip_files:
        print(f"å‘ç° {len(zip_files)} ä¸ªZIPæ–‡ä»¶ï¼Œå¼€å§‹è§£å‹...")
        
        for zip_path in zip_files:
            zip_name = os.path.splitext(os.path.basename(zip_path))[0]
            extract_path = os.path.join(data_dir, zip_name + "_extracted")
            
            try:
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    zip_ref.extractall(extract_path)
                    print(f"âœ… å·²è§£å‹: {os.path.basename(zip_path)} -> {zip_name}_extracted")
                    extracted_dirs.append((zip_name, extract_path))
            except Exception as e:
                print(f"âŒ è§£å‹å¤±è´¥: {os.path.basename(zip_path)}, é”™è¯¯: {e}")
    
    return extracted_dirs

def detect_case_structure(data_dir):
    """æ£€æµ‹ç›®å½•ç»“æ„ç±»å‹å¹¶è¿”å›caseåˆ—è¡¨"""
    cases = []
    
    # å…ˆå¤„ç†ZIPæ–‡ä»¶
    extracted_cases = extract_zip_files(data_dir)
    cases.extend(extracted_cases)
    
    # æ£€æŸ¥ç°æœ‰å­ç›®å½•
    subdirs = []
    root_dicom_files = []
    
    for item in os.listdir(data_dir):
        item_path = os.path.join(data_dir, item)
        if os.path.isdir(item_path) and not item.endswith("_extracted"):
            subdirs.append((item, item_path))
        elif os.path.isfile(item_path) and is_dicom_file(item_path):
            root_dicom_files.append(item_path)
    
    total_dirs = len(subdirs) + len(extracted_cases)
    
    if total_dirs > 0 and len(root_dicom_files) == 0:
        print(f"æ£€æµ‹åˆ°å¤šcaseæ¨¡å¼: å‘ç° {len(subdirs)} ä¸ªå­ç›®å½• + {len(extracted_cases)} ä¸ªè§£å‹ç›®å½•")
        cases.extend(subdirs)
    elif len(root_dicom_files) > 0 and total_dirs == 0:
        print(f"æ£€æµ‹åˆ°å•caseæ¨¡å¼: æ ¹ç›®å½•åŒ…å« {len(root_dicom_files)} ä¸ªDICOMæ–‡ä»¶")
        cases = [("single_case", data_dir)]
    elif len(root_dicom_files) > 0 and total_dirs > 0:
        print(f"æ£€æµ‹åˆ°æ··åˆæ¨¡å¼: {total_dirs} ä¸ªç›®å½• + {len(root_dicom_files)} ä¸ªæ ¹ç›®å½•DICOMæ–‡ä»¶")
        print("å°†æŒ‰å¤šcaseæ¨¡å¼å¤„ç†å­ç›®å½•")
        cases.extend(subdirs)
    else:
        print("æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„caseç»“æ„")
    
    return cases

# æ™ºèƒ½æ£€æµ‹ç›®å½•ç»“æ„
cases = detect_case_structure(DATA_DIR)
if not cases:
    print("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„caseç›®å½•æˆ–DICOMæ–‡ä»¶")
    sys.exit(1)

processed_count = 0

for case_name, case_path in cases:
    print(f"\n=== å¤„ç† {case_name} ===")
    
    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰DICOMæ–‡ä»¶
    dicom_files = find_dicom_files(case_path)
    print(f"æ‰¾åˆ° {len(dicom_files)} ä¸ªDICOMæ–‡ä»¶")
    
    if not dicom_files:
        print("æœªæ‰¾åˆ°ä»»ä½•DICOMæ–‡ä»¶ï¼Œè·³è¿‡ã€‚")
        continue
    
    # ç»Ÿè®¡æ¯ä¸ªåºåˆ—çš„åˆ‡ç‰‡æ•°
    series_dict = defaultdict(list)
    for fpath in dicom_files:
        try:
            ds = pydicom.dcmread(fpath, stop_before_pixels=True)
            series_uid = getattr(ds, 'SeriesInstanceUID', None)
            if series_uid:
                series_dict[series_uid].append(fpath)
        except Exception as e:
            continue
    
    if not series_dict:
        print("æœªæ‰¾åˆ°ä»»ä½•åºåˆ—ï¼Œè·³è¿‡ã€‚")
        continue
    
    series_count = len(series_dict)
    max_image_count = max([len(v) for v in series_dict.values()]) if series_dict else 0
    max_uid = max(series_dict, key=lambda k: len(series_dict[k]))
    max_files = series_dict[max_uid]
    print(f"æœ€å¤§åºåˆ— UID: {max_uid}ï¼Œåˆ‡ç‰‡æ•°: {len(max_files)}")
    
    # æå–å…ƒæ•°æ® - å¢å¼ºç‰ˆï¼Œå°è¯•å¤šä¸ªæ–‡ä»¶è·å–å®Œæ•´ä¿¡æ¯
    metadata_extracted = False
    files_to_try = [dicom_files[0]]  # å…ˆå°è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶
    
    # å¦‚æœæœ€å¤§åºåˆ—æ–‡ä»¶ä¸åŒï¼Œä¹Ÿå°è¯•æœ€å¤§åºåˆ—çš„ç¬¬ä¸€ä¸ªæ–‡ä»¶
    if max_files and max_files[0] != dicom_files[0]:
        files_to_try.append(max_files[0])
    
    # å†éšæœºå°è¯•å‡ ä¸ªæ–‡ä»¶ä½œä¸ºå¤‡é€‰
    if len(dicom_files) > 2:
        files_to_try.extend(dicom_files[1:min(4, len(dicom_files))])
    
    row = None
    for file_to_try in files_to_try:
        try:
            print(f"   å°è¯•ä»æ–‡ä»¶æå–å…ƒæ•°æ®: {os.path.basename(file_to_try)}")
            ds = pydicom.dcmread(file_to_try, stop_before_pixels=True)
            
            project_id = len(metadata_rows) + 1
            row = {
                'ProjectID': project_id,
                'FileName': case_name,
                'PatientName': getattr(ds, 'PatientName', ''),
                'PatientID': getattr(ds, 'PatientID', ''),
                'StudyDate': getattr(ds, 'StudyDate', ''),
                'PatientBirthDate': getattr(ds, 'PatientBirthDate', ''),
                'PatientAge': getattr(ds, 'PatientAge', ''),
                'PatientSex': getattr(ds, 'PatientSex', ''),
                'StudyInstanceUID': getattr(ds, 'StudyInstanceUID', ''),
                'SeriesInstanceUID': max_uid,
                'Modality': getattr(ds, 'Modality', ''),
                'Manufacturer': getattr(ds, 'Manufacturer', ''),
                'Rows': getattr(ds, 'Rows', ''),
                'Columns': getattr(ds, 'Columns', ''),
                'ImageCount': max_image_count,
                'SeriesCount': series_count
            }
            
            # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦æœ‰æ•ˆ
            critical_fields_ok = bool(row['PatientID'] or row['Modality'] or row['StudyInstanceUID'])
            if critical_fields_ok:
                print(f"   âœ… æˆåŠŸæå–å…ƒæ•°æ®: PatientID={row['PatientID']}, Modality={row['Modality']}")
                metadata_extracted = True
                break
            else:
                print(f"   âš ï¸ å…³é”®å…ƒæ•°æ®å­—æ®µä¸ºç©ºï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ–‡ä»¶...")
                
        except Exception as e:
            print(f"   âŒ å…ƒæ•°æ®æå–å¤±è´¥: {e}")
            continue
    
    if metadata_extracted and row:
        metadata_rows.append(row)
        
        masked_row = row.copy()
        masked_row['PatientName'] = desensitize_name(row['PatientName'])
        masked_rows.append(masked_row)
        
    else:
        print(f"   âš ï¸ æ‰€æœ‰æ–‡ä»¶çš„å…ƒæ•°æ®æå–éƒ½å¤±è´¥ï¼Œè·³è¿‡æ­¤caseçš„å…ƒæ•°æ®è®°å½•")
    
    # è½¬æ¢æœ€å¤§åºåˆ—
    if not max_files:
        print("æœªæ‰¾åˆ°æœ‰æ•ˆåºåˆ—ï¼Œè·³è¿‡è½¬æ¢")
        continue
        
    with tempfile.TemporaryDirectory() as tmpdir, tempfile.TemporaryDirectory() as tmpout:
        # å¤åˆ¶æœ€å¤§åºåˆ—æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        for f in max_files:
            shutil.copy2(f, tmpdir)
        
        # è½¬æ¢
        cmd = [DCM2NIIX_PATH, '-z', 'y', '-o', tmpout, tmpdir]
        print(f"è¿è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"   è¾“å…¥ç›®å½•: {tmpdir} ({len(max_files)} ä¸ªæ–‡ä»¶)")
        print(f"   è¾“å‡ºç›®å½•: {tmpout}")
        print(f"   å¼€å§‹è½¬æ¢ï¼Œè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
        
        try:
            # å¯¹äºå¤§æ•°æ®é›†ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´åˆ°10åˆ†é’Ÿ
            timeout_minutes = 10 if len(max_files) > 500 else 5
            print(f"   è®¾ç½®è¶…æ—¶: {timeout_minutes} åˆ†é’Ÿ")
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout_minutes * 60)
            
            print(f"   è½¬æ¢å®Œæˆï¼Œè¿”å›ç : {result.returncode}")
            
            if result.stdout:
                print(f"   æ ‡å‡†è¾“å‡º: {result.stdout.strip()}")
            
            if result.returncode != 0:
                print(f"   âŒ è½¬æ¢å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                if result.stderr:
                    print(f"   é”™è¯¯ä¿¡æ¯: {result.stderr.strip()}")
                continue
            else:
                print(f"   âœ… è½¬æ¢æˆåŠŸ")
                
        except subprocess.TimeoutExpired:
            print(f"   â° è½¬æ¢è¶…æ—¶ï¼ˆ{timeout_minutes}åˆ†é’Ÿï¼‰ï¼Œè·³è¿‡æ­¤case")
            continue
        except Exception as e:
            print(f"   âŒ è½¬æ¢å¼‚å¸¸: {e}")
            continue
        
        # åˆ é™¤JSONæ–‡ä»¶
        for jf in glob.glob(os.path.join(tmpout, '*.json')):
            os.remove(jf)
        
        # è·å–NIfTIæ–‡ä»¶
        nii_files = glob.glob(os.path.join(tmpout, '*.nii.gz'))
        if not nii_files:
            print("   æœªç”ŸæˆNIfTIæ–‡ä»¶")
            continue
        
        # é€‰æœ€å¤§åˆ‡ç‰‡æ•°æ–‡ä»¶
        max_slices = -1
        max_file = None
        for f in nii_files:
            try:
                img = nib.load(f)
                shape = img.shape
                slices = shape[2] if len(shape) >= 3 else 1
                if slices > max_slices:
                    max_slices = slices
                    max_file = f
            except Exception as e:
                continue
        
        if not max_file:
            print("   æ— æ³•ç¡®å®šæœ€å¤§åˆ‡ç‰‡æ–‡ä»¶")
            continue
        
        # å¤åˆ¶åˆ°outputç›®å½•
        target_name = os.path.join(OUTPUT_DIR, case_name + '.nii.gz')
        if os.path.exists(target_name):
            os.remove(target_name)
        shutil.copy2(max_file, target_name)
        print(f"   å·²è¾“å‡º: {case_name}.nii.gz ({max_slices} åˆ‡ç‰‡)")
        processed_count += 1

# ç”Ÿæˆå…ƒæ•°æ®CSVæ–‡ä»¶
print(f"\næ‰¹é‡å¤„ç†å®Œæˆï¼æ­£åœ¨ç”Ÿæˆå…ƒæ•°æ®æ–‡ä»¶...")

if metadata_rows:
    # åŸå§‹å…ƒæ•°æ®CSV
    with open(META_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ProjectID'] + FIELDS)
        for row in metadata_rows:
            writer.writerow([row[field] for field in ['ProjectID'] + FIELDS])
    print(f"âœ… å·²ç”Ÿæˆ: {META_CSV}")
    
    # è„±æ•å…ƒæ•°æ®CSV
    with open(MASKED_CSV, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow(['ProjectID'] + FIELDS)
        for row in masked_rows:
            writer.writerow([row[field] for field in ['ProjectID'] + FIELDS])
    print(f"âœ… å·²ç”Ÿæˆ: {MASKED_CSV}")
    
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"   æˆåŠŸå¤„ç†: {processed_count} ä¸ªcase")
    print(f"   è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
else:
    print("âš ï¸ æœªå¤„ç†ä»»ä½•æœ‰æ•ˆçš„case")